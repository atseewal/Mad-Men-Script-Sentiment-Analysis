{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13836a",
   "metadata": {},
   "source": [
    "# BART Mad Men Analysis\n",
    "\n",
    "Work done before this script:\n",
    "  * Blu Ray copied from disc to file\n",
    "  * Bitmap subtitles are extracted with ffmpeg\n",
    "  * On Linux machine, pgsrip (using Tesseract OCR) is used to extract text subtitles into .srt format\n",
    "\n",
    "This notebook:\n",
    "  * Iterate over directory of .srt files\n",
    "  * For each .srt file:\n",
    "    * Get the name of the file (this has the season and episode number)\n",
    "    * Use `clean_up_text()` to get a list of lines from the episode\n",
    "    * Load model to GPU\n",
    "    * Create hypothesis statements\n",
    "    * For each line in the episode, get probability for positive and negative\n",
    "    * Count times positive was greater than negative\n",
    "    * Return proportion of positive lines\n",
    "  * Web scrape the episode ratings for all seasons\n",
    "  * Assemble season/episode name, positive proportion, and rating to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659afd34",
   "metadata": {},
   "source": [
    "Get paths for all the subtitle files. Save it to a list: `script_file_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a2a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "scripts_path = '/home/cheetah/Documents/UWSP/DS 745/Scripts/Scripts - srt/'\n",
    "\n",
    "script_files = os.listdir(scripts_path)\n",
    "\n",
    "script_file_list = [f'{scripts_path}{script}' for script in script_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb69e5",
   "metadata": {},
   "source": [
    "Create the functions to extract text from .srt files.\n",
    "\n",
    "This also has some logic to re-assemble sentences that have been split and are displayed on separate lines when shown as captions on the video.\n",
    "\n",
    "The extraction and re-assembly of sentences code is closely based off the code here (with some small modifications):\n",
    "\n",
    "https://www.webucator.com/article/simple-python-script-for-extracting-text-from-an-s/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbd6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "\n",
    "def is_time_stamp(l):\n",
    "  if l[:2].isnumeric() and l[2] == ':':\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def has_letters(line):\n",
    "  if re.search('[a-zA-Z]', line):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def has_no_text(line):\n",
    "  l = line.strip()\n",
    "  if not len(l):\n",
    "    return True\n",
    "  if l.isnumeric():\n",
    "    return True\n",
    "  if is_time_stamp(l):\n",
    "    return True\n",
    "  if l[0] == '(' and l[-1] == ')':\n",
    "    return True\n",
    "  if not has_letters(line):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def is_lowercase_letter_or_comma(letter):\n",
    "  if letter.isalpha() and letter.lower() == letter:\n",
    "    return True\n",
    "  if letter == ',':\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def clean_up(lines):\n",
    "  \"\"\"\n",
    "  Get rid of all non-text lines and\n",
    "  try to combine text broken into multiple lines\n",
    "  \"\"\"\n",
    "  new_lines = []\n",
    "  for line in lines[1:]:\n",
    "    if has_no_text(line):\n",
    "      continue\n",
    "    elif len(new_lines) and is_lowercase_letter_or_comma(line[0]):\n",
    "      #combine with previous line\n",
    "      new_lines[-1] = new_lines[-1].strip() + ' ' + line\n",
    "    else:\n",
    "      #append line\n",
    "      new_lines.append(line)\n",
    "  return new_lines\n",
    "\n",
    "def clean_up_text(input_file, file_encoding='utf-8'):\n",
    "  \"\"\"\n",
    "    args[1]: file name\n",
    "    args[2]: encoding. Default: utf-8.\n",
    "      - If you get a lot of [?]s replacing characters,\n",
    "      - you probably need to change file_encoding to 'cp1252'\n",
    "  \"\"\"\n",
    "  file_name = input_file\n",
    "  with open(file_name, encoding=file_encoding, errors='replace') as f:\n",
    "    lines = f.readlines()\n",
    "    new_lines = clean_up(lines)\n",
    "  #with open(new_file_name, 'w') as f:\n",
    "  #  for line in new_lines:\n",
    "  #    f.write(line)\n",
    "\n",
    "  new_lines = [re.sub(r'\\n', '', line) for line in new_lines]\n",
    "\n",
    "  return new_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e727d63",
   "metadata": {},
   "source": [
    "Sets up the bart model.\n",
    "  * Use GPU 1. This is the second Nvidia RTX 3090 in the system. It's used because GPU 0 drives the monitors\n",
    "  * Create the positive and negative hypothesis strings. BART outputs the probability that these match the text string inputted.\n",
    "  * The model is loaded and sent to the GPU\n",
    "\n",
    "The script then iterates over each script in the script list.\n",
    "  * Get the episode name in sXXeXX format\n",
    "  * Use the `clean_up_text` function defined earlier to extract the text from the .srt script\n",
    "  * Take each sentence of the script\n",
    "    * Tokenize the text, sending the tokenized text to the GPU\n",
    "    * Get the probability output\n",
    "    * Append the probabilities to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2ea5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheetah/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "100%|███████████████████████████████████████████| 89/89 [40:36<00:00, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 41s, sys: 2.43 s, total: 40min 43s\n",
      "Wall time: 40min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "re_episode = re.compile(r'MMs\\d{2}e\\d{2}')\n",
    "\n",
    "episode_names = []\n",
    "positive_prob = []\n",
    "negative_prob = []\n",
    "\n",
    "import torch\n",
    "device = \"cuda:1\" #GPU that isn't outputting video\n",
    "\n",
    "hypothesis_positive = f'Positive'#This sentence is Positive'\n",
    "hypothesis_negative = f'Negative'#This sentence is Negative'\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "for script in tqdm(script_file_list):\n",
    "    # get file name\n",
    "    current_episode = re_episode.search(script).group(0)\n",
    "    #print(f'\\n\\n*********Current Episode: {current_episode}***********')\n",
    "    #episode_names.append(current_episode)\n",
    "    \n",
    "    # clean the episode text\n",
    "    clean_text_list = clean_up_text(script)\n",
    "    \n",
    "    for text in clean_text_list:\n",
    "        # Re-write it so that the larger probability statement is saved to a list\n",
    "        episode_names.append(current_episode)\n",
    "        x = tokenizer.encode(text, hypothesis_positive, return_tensors='pt', truncation=True)\n",
    "        logits = nli_model(x.to(device))[0]\n",
    "\n",
    "        entail_contradiction_logits = logits[:,[0,2]]\n",
    "        probs = entail_contradiction_logits.softmax(dim=1)\n",
    "        prob_label_is_positive = probs[:,1]\n",
    "\n",
    "        x = tokenizer.encode(text, hypothesis_negative, return_tensors='pt', truncation=True)\n",
    "        logits = nli_model(x.to(device))[0]\n",
    "\n",
    "        entail_contradiction_logits = logits[:,[0,2]]\n",
    "        probs = entail_contradiction_logits.softmax(dim=1)\n",
    "        prob_label_is_negative = probs[:,1]\n",
    "\n",
    "        positive_prob.append(prob_label_is_positive.item())\n",
    "        negative_prob.append(prob_label_is_negative.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e5f22",
   "metadata": {},
   "source": [
    "Create a pandas DataFrame from the output lists\n",
    "\n",
    "Create a new column of the DataFrame that has Positive and Negative values, based on which probability is higher\n",
    "\n",
    "Output a summary of Positive and Negative sentences for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8def2c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode_name  sentiment\n",
       "MMs01e01      Negative     404\n",
       "              Positive     337\n",
       "MMs01e02      Negative     412\n",
       "              Positive     268\n",
       "MMs01e03      Positive     341\n",
       "                          ... \n",
       "MMs07e12      Positive     357\n",
       "MMs07e13      Negative     470\n",
       "              Positive     320\n",
       "MMs07e14      Negative     475\n",
       "              Positive     303\n",
       "Length: 178, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "output = pd.DataFrame({'episode_name': episode_names, 'positive_prob': positive_prob, 'negative_prob': negative_prob})\n",
    "output['sentiment'] = np.where(output['positive_prob'] > output['negative_prob'], 'Positive', 'Negative')\n",
    "output.groupby('episode_name')[['episode_name', 'sentiment']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02628c",
   "metadata": {},
   "source": [
    "Output the data to .csv for analysis in Tableau or other software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde42e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"mad_men_text_analysis_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af3e16",
   "metadata": {},
   "source": [
    "## Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afecea",
   "metadata": {},
   "source": [
    "Define a function that will get the IMDB information for every episode given a title_id and season number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800a51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_imdb_episode_data(title_id, season):    \n",
    "    base_url = 'https://www.imdb.com/title/{}/episodes?season={}'.format(title_id, season)\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    odd_titles = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.odd div.info strong a')\n",
    "    even_titles = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.even div.info strong a')\n",
    "\n",
    "    odd_ratings = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.odd div.info div.ipl-rating-widget div.ipl-rating-star.small span.ipl-rating-star__rating')\n",
    "    even_ratings = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.even div.info div.ipl-rating-widget div.ipl-rating-star.small span.ipl-rating-star__rating')\n",
    "\n",
    "    odd_airdate = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.odd div.info div.airdate')\n",
    "    even_airdate = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.even div.info div.airdate')\n",
    "\n",
    "    odd_descriptions = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.odd div.info div.item_description')\n",
    "    even_descriptions = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.even div.info div.item_description')\n",
    "\n",
    "    odd_episode_numbers = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.odd div.image a div.hover-over-image.zero-z-index div')\n",
    "    even_episode_numbers = soup.select('#styleguide-v2.fixed div#wrapper div#root.redesign div#pagecontent.pagecontent div#content-2-wide.redesign div#main div.article.listo.list div#episodes_content.header div.clear div.list.detail.eplist div.list_item.even div.image a div.hover-over-image.zero-z-index div')\n",
    "    \n",
    "    odd_titles = [title.text for title in odd_titles]\n",
    "    even_titles = [title.text for title in even_titles]\n",
    "\n",
    "    odd_ratings = [rating.text for rating in odd_ratings]\n",
    "    even_ratings = [rating.text for rating in even_ratings]\n",
    "\n",
    "    odd_airdate = [airdate.text for airdate in odd_airdate]\n",
    "    even_airdate = [airdate.text for airdate in even_airdate]\n",
    "\n",
    "    odd_descriptions = [description.text for description in odd_descriptions]\n",
    "    even_descriptions = [description.text for description in even_descriptions]\n",
    "\n",
    "    odd_episode_numbers = [episode_number.text for episode_number in odd_episode_numbers]\n",
    "    even_episode_numbers = [episode_number.text for episode_number in even_episode_numbers]\n",
    "    \n",
    "    episodes = list(zip(odd_titles, odd_ratings, odd_airdate, odd_descriptions, odd_episode_numbers))\n",
    "    episodes.extend(list(zip(even_titles, even_ratings, even_airdate, even_descriptions, even_episode_numbers)))\n",
    "    \n",
    "    # Make DF: episodes_df\n",
    "    episodes_df = pd.DataFrame(episodes, columns=['episode_name', 'rating', 'air_date', 'description', 'episode_number'])\n",
    "    \n",
    "    # Clean up new line characters\n",
    "    episodes_df['air_date'] = episodes_df['air_date'].str.replace(r'\\n', '', regex=True)\n",
    "    episodes_df['description'] = episodes_df['description'].str.replace(r'\\n', '', regex=True)\n",
    "    \n",
    "    # return episodes_df\n",
    "    return episodes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a04e4",
   "metadata": {},
   "source": [
    "The episode numbers are corrected by hand after the output file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d44022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:43<00:00,  6.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>air_date</th>\n",
       "      <th>description</th>\n",
       "      <th>episode_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Time Zones</td>\n",
       "      <td>7.9</td>\n",
       "      <td>13 Apr. 2014</td>\n",
       "      <td>Don is on the outside looking in on the forced...</td>\n",
       "      <td>S7, Ep1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Field Trip</td>\n",
       "      <td>8.5</td>\n",
       "      <td>27 Apr. 2014</td>\n",
       "      <td>After a fight with Megan during a surprise tri...</td>\n",
       "      <td>S7, Ep3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Runaways</td>\n",
       "      <td>8.5</td>\n",
       "      <td>11 May 2014</td>\n",
       "      <td>While Lou becomes the object of ridicule for t...</td>\n",
       "      <td>S7, Ep5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waterloo</td>\n",
       "      <td>9.5</td>\n",
       "      <td>25 May 2014</td>\n",
       "      <td>Don and Peggy prepare to pitch to Burger Chef,...</td>\n",
       "      <td>S7, Ep7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Business</td>\n",
       "      <td>7.8</td>\n",
       "      <td>12 Apr. 2015</td>\n",
       "      <td>While Megan comes to New York with her mother ...</td>\n",
       "      <td>S7, Ep9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>To Have and to Hold</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21 Apr. 2013</td>\n",
       "      <td>Don works in secret on a Heinz ketchup campaig...</td>\n",
       "      <td>S6, Ep4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For Immediate Release</td>\n",
       "      <td>8.8</td>\n",
       "      <td>5 May 2013</td>\n",
       "      <td>As the firm prepares to go public, Don and Pet...</td>\n",
       "      <td>S6, Ep6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Crash</td>\n",
       "      <td>8.7</td>\n",
       "      <td>19 May 2013</td>\n",
       "      <td>The creative department has a wild, drug-influ...</td>\n",
       "      <td>S6, Ep8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A Tale of Two Cities</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2 Jun. 2013</td>\n",
       "      <td>Cutler and Chaough prepare to make radical cha...</td>\n",
       "      <td>S6, Ep10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Quality of Mercy</td>\n",
       "      <td>8.5</td>\n",
       "      <td>16 Jun. 2013</td>\n",
       "      <td>Don renews his battle with Ted after seeing hi...</td>\n",
       "      <td>S6, Ep12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode_name rating                      air_date  \\\n",
       "0              Time Zones    7.9              13 Apr. 2014       \n",
       "1              Field Trip    8.5              27 Apr. 2014       \n",
       "2            The Runaways    8.5               11 May 2014       \n",
       "3                Waterloo    9.5               25 May 2014       \n",
       "4            New Business    7.8              12 Apr. 2015       \n",
       "..                    ...    ...                           ...   \n",
       "8     To Have and to Hold    8.0              21 Apr. 2013       \n",
       "9   For Immediate Release    8.8                5 May 2013       \n",
       "10              The Crash    8.7               19 May 2013       \n",
       "11   A Tale of Two Cities    8.1               2 Jun. 2013       \n",
       "12   The Quality of Mercy    8.5              16 Jun. 2013       \n",
       "\n",
       "                                          description episode_number  \n",
       "0   Don is on the outside looking in on the forced...        S7, Ep1  \n",
       "1   After a fight with Megan during a surprise tri...        S7, Ep3  \n",
       "2   While Lou becomes the object of ridicule for t...        S7, Ep5  \n",
       "3   Don and Peggy prepare to pitch to Burger Chef,...        S7, Ep7  \n",
       "4   While Megan comes to New York with her mother ...        S7, Ep9  \n",
       "..                                                ...            ...  \n",
       "8   Don works in secret on a Heinz ketchup campaig...        S6, Ep4  \n",
       "9   As the firm prepares to go public, Don and Pet...        S6, Ep6  \n",
       "10  The creative department has a wild, drug-influ...        S6, Ep8  \n",
       "11  Cutler and Chaough prepare to make radical cha...       S6, Ep10  \n",
       "12  Don renews his battle with Ted after seeing hi...       S6, Ep12  \n",
       "\n",
       "[92 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mad_men_list = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "for i in tqdm(range(7)):\n",
    "    mad_men_list.append(get_imdb_episode_data('tt0804503', i))\n",
    "    time.sleep(5)\n",
    "\n",
    "mad_men_data = pd.concat(mad_men_list)\n",
    "\n",
    "mad_men_data.to_csv(\"IMDB_ratings.csv\", index=False)\n",
    "\n",
    "mad_men_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
